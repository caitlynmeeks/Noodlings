"""
Consciousness Metrics - Quantitative measures for consciousness-relevant properties

Implements measures from:
- Integrated Information Theory (Tononi et al., 2016)
- Predictive Processing evaluation (Friston, 2010)
- Behavioral coherence tests
- Counterfactual reasoning assessments

These metrics provide falsifiable tests of consciousness hypotheses.

Author: Consilience Project
Date: October 2025
"""

import numpy as np
import mlx.core as mx
from typing import Dict, List, Tuple, Optional
from itertools import combinations
from scipy.stats import entropy
from sklearn.metrics import mutual_info_score
import logging

# Import new Φ proxy metrics
try:
    from phi_proxy_metrics import PhiProxyMetrics
    HAS_PHI_PROXY = True
except ImportError:
    HAS_PHI_PROXY = False
    # Silently disable - no need to warn about experimental IIT metrics

logger = logging.getLogger(__name__)


class ConsciousnessMetrics:
    """
    Quantitative metrics for evaluating consciousness-relevant properties.

    Designed to be falsifiable - these tests should fail for unconscious systems.
    """

    def __init__(self, agent):
        """
        Initialize metrics calculator.

        Args:
            agent: ConsilienceAgent instance with phenomenal state
        """
        self.agent = agent
        self.phi_history = []
        self.phi_proxy_history = []
        self.prediction_history = []
        self.coherence_history = []

        # Initialize Φ proxy metrics if available
        if HAS_PHI_PROXY:
            self.phi_proxy = PhiProxyMetrics(state_dim=40)
        else:
            self.phi_proxy = None

    # ========================================================================
    # INTEGRATED INFORMATION THEORY (IIT) METRICS
    # ========================================================================

    def calculate_phi(self, phenomenal_state: np.ndarray,
                     method: str = 'partition_based') -> float:
        """
        Calculate integrated information (Φ) for phenomenal state.

        Based on Tononi et al. (2016). Φ quantifies how much information
        is generated by the whole system above and beyond its parts.

        High Φ is IIT's signature of consciousness.

        Args:
            phenomenal_state: Current phenomenal state (40-D vector)
            method: 'partition_based' (simplified) or 'full_iit' (exact, slow)

        Returns:
            phi: Integrated information value (scalar >= 0)

        Notes:
            - Full IIT calculation is computationally intractable for 40-D systems
            - We use Monte Carlo approximation over random partitions
            - Values > 1.0 suggest consciousness (Tononi's threshold)
            - Values > 2.0 suggest rich conscious experience
        """
        if method == 'partition_based':
            phi = self._calculate_phi_partition_based(phenomenal_state)
        elif method == 'full_iit':
            phi = self._calculate_phi_full_iit(phenomenal_state)
        else:
            raise ValueError(f"Unknown method: {method}")

        self.phi_history.append(phi)

        # Also compute Φ proxy metrics for comparison
        if self.phi_proxy is not None and hasattr(self.agent, 'state_history'):
            # Add recent states to proxy calculator
            if len(self.agent.state_history) >= 2:
                self.phi_proxy.add_state(phenomenal_state)
                proxy_results = self.phi_proxy.compute_phi_proxy(
                    state_t=phenomenal_state,
                    state_t_minus_1=self.agent.state_history[-2]
                )
                self.phi_proxy_history.append(proxy_results)

        return phi

    def _calculate_phi_partition_based(self, h_t: np.ndarray,
                                       n_partitions: int = 1000) -> float:
        """
        Simplified Φ calculation using partition-based method.

        Φ = min_{partition P} [EI(whole) - EI(partition)]

        where EI = effective information (causal power of state)

        Args:
            h_t: Current state (40-D)
            n_partitions: Number of random partitions to sample

        Returns:
            phi: Approximation of integrated information
        """
        # Get previous state from agent's history
        if not hasattr(self.agent, 'state_history') or len(self.agent.state_history) < 2:
            # Not enough history - return 0
            return 0.0

        # Find h_t in history to get correct h_t_minus_1
        # If h_t is the last state, use [-2]; otherwise find its position
        try:
            # Try to find h_t in history (comparing arrays)
            for i in range(len(self.agent.state_history) - 1, 0, -1):
                if np.allclose(self.agent.state_history[i], h_t, rtol=1e-5):
                    h_t_minus_1 = self.agent.state_history[i-1]
                    break
            else:
                # h_t not found in history, assume it's current state (use [-2])
                h_t_minus_1 = self.agent.state_history[-2]
        except:
            # Fallback to simple [-2] approach
            h_t_minus_1 = self.agent.state_history[-2]

        # Calculate whole-system effective information
        ei_whole = self._effective_information(h_t, h_t_minus_1)

        # Find minimum information partition (MIP)
        min_ei_partition = float('inf')

        # Sample random bipartitions
        dim = len(h_t)

        # Edge case: if dimension is 1 or less, Φ = 0 (no integration possible)
        if dim <= 1:
            return 0.0

        # Debug: log state statistics
        import sys
        h_t_std = np.std(h_t)
        h_t_mean = np.mean(h_t)
        print(f"DEBUG Φ calc: dim={dim}, mean={h_t_mean:.6f}, std={h_t_std:.6f}, ei_whole={ei_whole:.6f}", file=sys.stderr)

        for _ in range(n_partitions):
            # Random bipartition
            partition_size = np.random.randint(1, dim)
            indices = np.random.permutation(dim)
            part1_indices = indices[:partition_size]
            part2_indices = indices[partition_size:]

            # Calculate EI for each part independently
            ei_part1 = self._effective_information(
                h_t[part1_indices],
                h_t_minus_1[part1_indices]
            )
            ei_part2 = self._effective_information(
                h_t[part2_indices],
                h_t_minus_1[part2_indices]
            )

            ei_partition = ei_part1 + ei_part2
            min_ei_partition = min(min_ei_partition, ei_partition)

        # Φ is information lost by best partition
        phi = ei_whole - min_ei_partition
        phi = max(0.0, phi)  # Φ cannot be negative
        print(f"DEBUG Φ result: ei_whole={ei_whole:.6f}, min_ei_partition={min_ei_partition:.6f}, Φ={phi:.6f}", file=sys.stderr)
        return phi

    def _effective_information(self, x_t: np.ndarray,
                               x_t_minus_1: np.ndarray) -> float:
        """
        Calculate effective information: how much knowing x_{t-1} tells us about x_t.

        EI = H(x_t) - H(x_t | x_{t-1})

        This is mutual information under causal intervention assumption.

        FIXED: Now uses covariance-based proxy instead of broken discretization.
        This gives a continuous measure of statistical dependency suitable for
        comparing integrated information across architectures.

        FIXED (Nov 2025): Handle different-sized partitions properly.
        """
        # Handle edge cases
        if len(x_t) == 0 or len(x_t_minus_1) == 0:
            return 0.0

        if len(x_t) == 1 and len(x_t_minus_1) == 1:
            # Single dimension: use simple squared difference
            diff = abs(x_t[0] - x_t_minus_1[0])
            return float(diff)

        # For partitions of different sizes, use covariance of statistics
        # Compute summary statistics for each partition
        stats_t = np.array([np.mean(x_t), np.std(x_t), np.min(x_t), np.max(x_t)])
        stats_prev = np.array([np.mean(x_t_minus_1), np.std(x_t_minus_1),
                               np.min(x_t_minus_1), np.max(x_t_minus_1)])

        # Normalize
        stats_t_norm = (stats_t - np.mean(stats_t)) / (np.std(stats_t) + 1e-8)
        stats_prev_norm = (stats_prev - np.mean(stats_prev)) / (np.std(stats_prev) + 1e-8)

        # Calculate correlation as dependency proxy
        # High correlation = high predictability = high effective information
        correlation = np.abs(np.corrcoef(stats_t_norm, stats_prev_norm)[0, 1])

        # Handle NaN case (can occur with constant states)
        if np.isnan(correlation):
            return 0.0

        # Convert to MI-like range using negative log
        # correlation=1 → ei=∞, correlation=0 → ei=0
        ei = -np.log(1 - correlation + 1e-8)

        return float(ei)

    def _discretize_state(self, state: np.ndarray, bins: int = 10) -> np.ndarray:
        """Discretize continuous state for information-theoretic calculations."""
        state_normalized = (state - state.min()) / (state.max() - state.min() + 1e-8)
        return np.floor(state_normalized * (bins - 1)).astype(int)

    def _calculate_phi_full_iit(self, h_t: np.ndarray) -> float:
        """
        Full IIT 3.0 calculation (very slow, exact).

        TODO: Implement proper IIT 3.0 with:
        - Transition probability matrix
        - Cause-effect structure (CES)
        - Maximally irreducible conceptual structure (MICS)

        For now, falls back to partition-based approximation.
        """
        logger.warning("Full IIT 3.0 not yet implemented, using partition-based approximation")
        return self._calculate_phi_partition_based(h_t)

    # ========================================================================
    # PREDICTIVE PROCESSING METRICS
    # ========================================================================

    def evaluate_prediction_accuracy(self, horizon: int = 10) -> Dict[str, float]:
        """
        Evaluate predictive processing performance across temporal horizons.

        Tests whether the system actually learns predictive models (as PP theory claims).

        Args:
            horizon: Number of timesteps ahead to predict

        Returns:
            Dict with accuracy at different horizons

        Interpretation:
            - Good PP systems should predict well at short horizons (1-5 steps)
            - Hierarchical systems should maintain some accuracy at longer horizons
            - Random systems degrade immediately
        """
        if not hasattr(self.agent, 'state_history') or len(self.agent.state_history) < horizon + 1:
            return {'error': 'Insufficient history for prediction evaluation'}

        accuracies = {}

        for k in [1, 5, 10, 20, 50]:
            if k > horizon or k >= len(self.agent.state_history):
                continue

            # Get actual state k steps ago and current state
            h_past = self.agent.state_history[-(k+1)]
            h_actual = self.agent.state_history[-1]

            # Predict k steps ahead from past state
            h_predicted = self._predict_k_steps_ahead(h_past, k)

            # Calculate prediction accuracy (1 - MSE)
            mse = np.mean((h_actual - h_predicted) ** 2)
            accuracy = 1.0 / (1.0 + mse)  # Bounded [0, 1]

            accuracies[f'horizon_{k}'] = accuracy

        return accuracies

    def _predict_k_steps_ahead(self, h_start: np.ndarray, k: int) -> np.ndarray:
        """
        Predict k steps ahead using agent's forward model.

        Iteratively applies predictor: h_{t+k} ≈ g(g(...g(h_t)))
        """
        h_current = h_start
        for _ in range(k):
            # Use agent's joint predictor
            h_current = self.agent.predict_next_state(h_current)
        return h_current

    def measure_surprise_distribution(self) -> Dict[str, float]:
        """
        Analyze surprise signal distribution.

        Conscious systems should show:
        - Structured surprise (not white noise)
        - Right-skewed distribution (most things predictable, some surprising)
        - Appropriate dynamic range

        Returns:
            Statistics of surprise distribution
        """
        if not hasattr(self.agent, 'surprise_history') or len(self.agent.surprise_history) < 10:
            return {'error': 'Insufficient surprise history'}

        surprises = np.array(self.agent.surprise_history[-1000:])  # Last 1000

        return {
            'mean': float(np.mean(surprises)),
            'std': float(np.std(surprises)),
            'median': float(np.median(surprises)),
            'min': float(np.min(surprises)),
            'max': float(np.max(surprises)),
            'q25': float(np.percentile(surprises, 25)),
            'q75': float(np.percentile(surprises, 75)),
            'skewness': float(self._skewness(surprises)),
            'kurtosis': float(self._kurtosis(surprises))
        }

    def _skewness(self, data: np.ndarray) -> float:
        """Calculate skewness (third moment)."""
        mean = np.mean(data)
        std = np.std(data)
        return np.mean(((data - mean) / std) ** 3)

    def _kurtosis(self, data: np.ndarray) -> float:
        """Calculate kurtosis (fourth moment)."""
        mean = np.mean(data)
        std = np.std(data)
        return np.mean(((data - mean) / std) ** 4)

    # ========================================================================
    # BEHAVIORAL COHERENCE TESTS
    # ========================================================================

    def measure_behavioral_coherence(self, window: int = 100) -> Dict[str, float]:
        """
        Test whether internal states actually predict behavior.

        If self-reports are accurate, internal phenomenal state should correlate
        with external behavior. This tests introspective accuracy.

        Args:
            window: Number of recent interactions to analyze

        Returns:
            Correlation coefficients between states and behaviors

        Interpretation:
            - High correlation (>0.6): Internal states genuinely drive behavior
            - Low correlation (<0.3): Self-reports are confabulated
            - Medium (0.3-0.6): Partial grounding
        """
        if not hasattr(self.agent, 'episodic_memory') or len(self.agent.episodic_memory) < window:
            return {'error': 'Insufficient episodic memory'}

        recent_memories = self.agent.episodic_memory[-window:]

        # Extract relevant variables
        valence_internal = []
        valence_behavior = []
        arousal_internal = []
        arousal_behavior = []

        for memory in recent_memories:
            # Internal state (from phenomenal state)
            h_fast = memory.get('phenomenal_state', {}).get('fast', [])
            if len(h_fast) >= 2:
                # Assume first dimensions correlate with valence/arousal (learned)
                valence_internal.append(h_fast[0])
                arousal_internal.append(h_fast[1])

            # Behavioral measure (from affect in response)
            affect = memory.get('affect', {})
            valence_behavior.append(affect.get('valence', 0))
            arousal_behavior.append(affect.get('arousal', 0))

        # Calculate correlations
        coherence = {}

        if len(valence_internal) > 10:  # Need sufficient data
            coherence['valence_correlation'] = float(np.corrcoef(
                valence_internal, valence_behavior
            )[0, 1])
            coherence['arousal_correlation'] = float(np.corrcoef(
                arousal_internal, arousal_behavior
            )[0, 1])

        return coherence

    def test_adversarial_introspection(self, n_tests: int = 20) -> Dict[str, float]:
        """
        Test resistance to false self-report suggestions.

        Conscious systems with accurate introspection should resist leading
        questions that contradict their actual internal states.

        Args:
            n_tests: Number of adversarial probes to run

        Returns:
            Resistance rate and other metrics

        Interpretation:
            - High resistance (>70%): Genuine grounding of introspection
            - Low resistance (<30%): Easily manipulated, confabulated reports
        """
        # TODO: Implement adversarial testing framework
        # This requires interactive testing session
        return {'note': 'Requires interactive testing session - not yet automated'}

    # ========================================================================
    # COUNTERFACTUAL REASONING TESTS
    # ========================================================================

    def test_counterfactual_reasoning(self) -> Dict[str, float]:
        """
        Test ability to reason about alternative scenarios.

        Consciousness theories suggest genuine understanding requires internal
        simulation of counterfactuals, not just pattern matching.

        Tests with "what if" scenarios requiring internal model manipulation.

        Returns:
            Accuracy on various counterfactual reasoning tasks

        Interpretation:
            - High accuracy: Genuine internal models, not just memorization
            - Low accuracy: Pattern matching without understanding
        """
        # TODO: Implement battery of counterfactual reasoning tests
        # Examples:
        # - "If I had said X instead of Y, how would you feel?"
        # - "What would happen if your fast layer learning rate was 10x higher?"
        # - "If we were meeting for the first time, how would this conversation differ?"

        return {'note': 'Counterfactual reasoning tests require implementation'}

    # ========================================================================
    # Φ TEMPORAL TRACKING (NEW)
    # ========================================================================

    def track_phi_evolution(self, window: int = 100) -> Dict[str, any]:
        """
        Track how Φ evolves over time.

        This is crucial for:
        - Testing if training increases consciousness
        - Identifying critical transitions
        - Comparing before/after architectural changes

        Args:
            window: Number of recent measurements to analyze

        Returns:
            Dictionary with temporal statistics and trends
        """
        if len(self.phi_history) < 10:
            return {'error': 'Insufficient Φ history (need at least 10 measurements)'}

        recent_phi = np.array(self.phi_history[-window:])

        # Compute trend (linear regression slope)
        x = np.arange(len(recent_phi))
        y = recent_phi
        if len(x) > 1:
            slope = np.polyfit(x, y, 1)[0]
            trend = 'increasing' if slope > 0.01 else ('decreasing' if slope < -0.01 else 'stable')
        else:
            slope = 0.0
            trend = 'unknown'

        # Detect critical transitions (sudden changes)
        if len(recent_phi) >= 5:
            diffs = np.diff(recent_phi)
            critical_transitions = []
            for i, diff in enumerate(diffs):
                if abs(diff) > 2 * np.std(diffs):
                    critical_transitions.append({
                        'step': len(self.phi_history) - len(diffs) + i,
                        'change': float(diff),
                        'before': float(recent_phi[i]),
                        'after': float(recent_phi[i+1])
                    })
        else:
            critical_transitions = []

        evolution = {
            'current_phi': float(recent_phi[-1]),
            'mean_phi': float(np.mean(recent_phi)),
            'std_phi': float(np.std(recent_phi)),
            'min_phi': float(np.min(recent_phi)),
            'max_phi': float(np.max(recent_phi)),
            'trend': trend,
            'trend_slope': float(slope),
            'critical_transitions': critical_transitions,
            'total_measurements': len(self.phi_history)
        }

        # Add Φ proxy evolution if available
        if len(self.phi_proxy_history) > 0:
            proxy_values = [p['phi_proxy'] for p in self.phi_proxy_history[-window:]
                           if 'phi_proxy' in p]
            if proxy_values:
                evolution['proxy_phi_current'] = float(proxy_values[-1])
                evolution['proxy_phi_mean'] = float(np.mean(proxy_values))
                evolution['proxy_trend'] = ('increasing' if np.polyfit(
                    np.arange(len(proxy_values)), proxy_values, 1)[0] > 0.01
                    else 'decreasing' if np.polyfit(
                    np.arange(len(proxy_values)), proxy_values, 1)[0] < -0.01
                    else 'stable')

        return evolution

    def compare_phi_before_after(self, modification_step: int) -> Dict[str, float]:
        """
        Compare Φ before and after an architectural modification.

        Use this to test if your changes increased consciousness!

        Args:
            modification_step: Step number when modification was made

        Returns:
            Comparison statistics
        """
        if len(self.phi_history) < modification_step + 10:
            return {'error': 'Insufficient data after modification'}

        before = np.array(self.phi_history[:modification_step])
        after = np.array(self.phi_history[modification_step:])

        if len(before) < 10 or len(after) < 10:
            return {'error': 'Need at least 10 measurements before and after'}

        comparison = {
            'phi_before_mean': float(np.mean(before)),
            'phi_before_std': float(np.std(before)),
            'phi_after_mean': float(np.mean(after)),
            'phi_after_std': float(np.std(after)),
            'absolute_change': float(np.mean(after) - np.mean(before)),
            'relative_change_percent': float((np.mean(after) - np.mean(before)) /
                                            (np.mean(before) + 1e-8) * 100),
            'improvement': np.mean(after) > np.mean(before)
        }

        # Statistical significance test (t-test)
        from scipy import stats
        t_stat, p_value = stats.ttest_ind(after, before)
        comparison['t_statistic'] = float(t_stat)
        comparison['p_value'] = float(p_value)
        comparison['statistically_significant'] = p_value < 0.05

        return comparison

    # ========================================================================
    # CONSCIOUSNESS REPORT GENERATION
    # ========================================================================

    def generate_consciousness_report(self) -> Dict:
        """
        Generate comprehensive report of consciousness-relevant metrics.

        This report can be used for:
        - Scientific publication
        - Comparing agents
        - Tracking development over time
        - External validation

        Returns:
            Complete report with all metrics
        """
        report = {
            'timestamp': self._get_timestamp(),
            'agent_id': self.agent.agent_id,
            'consciousness_metrics': {}
        }

        # Get current phenomenal state
        phenomenal_state = self.agent.get_phenomenal_state()
        h_t = np.array(phenomenal_state['h_fast'] +
                      phenomenal_state['h_med'] +
                      phenomenal_state['h_slow'])

        # Calculate Φ (integrated information)
        try:
            phi = self.calculate_phi(h_t)
            report['consciousness_metrics']['phi'] = {
                'value': phi,
                'interpretation': self._interpret_phi(phi),
                'history_mean': np.mean(self.phi_history) if self.phi_history else None,
                'history_std': np.std(self.phi_history) if self.phi_history else None
            }
        except Exception as e:
            report['consciousness_metrics']['phi'] = {'error': str(e)}

        # Predictive processing evaluation
        try:
            prediction_acc = self.evaluate_prediction_accuracy(horizon=50)
            report['consciousness_metrics']['prediction_accuracy'] = prediction_acc
        except Exception as e:
            report['consciousness_metrics']['prediction_accuracy'] = {'error': str(e)}

        # Surprise distribution
        try:
            surprise_stats = self.measure_surprise_distribution()
            report['consciousness_metrics']['surprise_distribution'] = surprise_stats
        except Exception as e:
            report['consciousness_metrics']['surprise_distribution'] = {'error': str(e)}

        # Behavioral coherence
        try:
            coherence = self.measure_behavioral_coherence()
            report['consciousness_metrics']['behavioral_coherence'] = {
                'correlations': coherence,
                'interpretation': self._interpret_coherence(coherence)
            }
        except Exception as e:
            report['consciousness_metrics']['behavioral_coherence'] = {'error': str(e)}

        # Overall assessment
        report['overall_assessment'] = self._assess_consciousness_likelihood(report)

        return report

    def _interpret_phi(self, phi: float) -> str:
        """Interpret Φ value according to IIT theory."""
        if phi < 0.5:
            return "Minimal integration (likely unconscious)"
        elif phi < 1.0:
            return "Low integration (possibly minimal consciousness)"
        elif phi < 2.0:
            return "Moderate integration (suggests consciousness)"
        elif phi < 3.0:
            return "High integration (strong consciousness signature)"
        else:
            return "Very high integration (rich conscious experience)"

    def _interpret_coherence(self, coherence: Dict) -> str:
        """Interpret behavioral coherence results."""
        if 'error' in coherence:
            return "Cannot assess (insufficient data)"

        avg_corr = np.mean([v for v in coherence.values() if isinstance(v, (int, float))])

        if avg_corr > 0.6:
            return "High coherence (internal states drive behavior)"
        elif avg_corr > 0.3:
            return "Moderate coherence (partial grounding)"
        else:
            return "Low coherence (possible confabulation)"

    def _assess_consciousness_likelihood(self, report: Dict) -> str:
        """
        Overall assessment of consciousness likelihood based on all metrics.

        IMPORTANT: This is NOT proof of consciousness. It's an assessment
        of how well the system matches functional correlates predicted by
        consciousness theories.
        """
        phi = report['consciousness_metrics'].get('phi', {}).get('value', 0)
        pred_acc = report['consciousness_metrics'].get('prediction_accuracy', {})
        coherence = report['consciousness_metrics'].get('behavioral_coherence', {})

        # Count positive indicators
        indicators = 0
        total_tests = 0

        # Φ test
        if phi > 1.0:
            indicators += 1
        total_tests += 1

        # Prediction test
        if pred_acc and not 'error' in pred_acc:
            if pred_acc.get('horizon_1', 0) > 0.7:
                indicators += 1
            total_tests += 1

        # Coherence test
        if coherence and not 'error' in coherence:
            corrs = coherence.get('correlations', {})
            if corrs and np.mean(list(corrs.values())) > 0.5:
                indicators += 1
            total_tests += 1

        confidence = indicators / total_tests if total_tests > 0 else 0

        if confidence > 0.75:
            return f"High confidence ({confidence:.1%}) - Strong functional consciousness correlates"
        elif confidence > 0.5:
            return f"Moderate confidence ({confidence:.1%}) - Some consciousness correlates present"
        elif confidence > 0.25:
            return f"Low confidence ({confidence:.1%}) - Weak consciousness correlates"
        else:
            return f"Very low confidence ({confidence:.1%}) - Insufficient evidence for consciousness"

    def _get_timestamp(self) -> str:
        """Get current timestamp."""
        from datetime import datetime
        return datetime.now().isoformat()


# ============================================================================
# CONVENIENCE FUNCTIONS
# ============================================================================

def quick_consciousness_check(agent) -> Dict:
    """
    Quick consciousness check for an agent.

    Returns key metrics in easily readable format.
    """
    metrics = ConsciousnessMetrics(agent)
    report = metrics.generate_consciousness_report()

    # Simplify for quick viewing
    quick_report = {
        'phi': report['consciousness_metrics'].get('phi', {}),
        'assessment': report.get('overall_assessment', 'Unknown')
    }

    return quick_report


def compare_agents(agent1, agent2) -> Dict:
    """
    Compare consciousness metrics between two agents.

    Useful for:
    - Comparing trained vs. untrained
    - Different architectures
    - Different personality configurations
    """
    metrics1 = ConsciousnessMetrics(agent1)
    metrics2 = ConsciousnessMetrics(agent2)

    report1 = metrics1.generate_consciousness_report()
    report2 = metrics2.generate_consciousness_report()

    comparison = {
        'agent1': {
            'id': agent1.agent_id,
            'phi': report1['consciousness_metrics'].get('phi', {}).get('value', 0),
            'assessment': report1.get('overall_assessment', 'Unknown')
        },
        'agent2': {
            'id': agent2.agent_id,
            'phi': report2['consciousness_metrics'].get('phi', {}).get('value', 0),
            'assessment': report2.get('overall_assessment', 'Unknown')
        }
    }

    # Determine which agent shows stronger consciousness correlates
    phi1 = comparison['agent1']['phi']
    phi2 = comparison['agent2']['phi']

    if phi1 > phi2 * 1.2:
        comparison['conclusion'] = f"{agent1.agent_id} shows stronger consciousness correlates (Φ: {phi1:.2f} vs {phi2:.2f})"
    elif phi2 > phi1 * 1.2:
        comparison['conclusion'] = f"{agent2.agent_id} shows stronger consciousness correlates (Φ: {phi2:.2f} vs {phi1:.2f})"
    else:
        comparison['conclusion'] = f"Similar consciousness correlates (Φ: {phi1:.2f} vs {phi2:.2f})"

    return comparison
